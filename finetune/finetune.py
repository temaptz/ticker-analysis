from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from datasets import load_dataset
from peft import LoraConfig, get_peft_model
import os
import sys
sys.stdout.reconfigure(line_buffering=True)


def tokenize_function(record):
    prompt = f'''
–¢—ã ‚Äî —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –û—Ü–µ–Ω–∏ —Å–ª–µ–¥—É—é—â—É—é –Ω–æ–≤–æ—Å—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –µ—ë –≤–ª–∏—è–Ω–∏—è –Ω–∞ —Å—Ç–æ–∏–º–æ—Å—Ç—å –∞–∫—Ü–∏–π –∫–æ–º–ø–∞–Ω–∏–∏ {record['subject_name']}.

–ó–∞–ø–æ–ª–Ω–∏ —Ç—Ä–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞:

1. sentiment ‚Äî —Ç–æ–Ω –Ω–æ–≤–æ—Å—Ç–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –∫–æ–º–ø–∞–Ω–∏–∏ (–æ—Ç -1.0 –¥–æ +1.0)
2. impact_strength ‚Äî —Å–∏–ª–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –≤–ª–∏—è–Ω–∏—è –Ω–∞ —Ü–µ–Ω—É –∞–∫—Ü–∏–π (–æ—Ç 0.0 –¥–æ 1.0)
3. mention_focus ‚Äî –Ω–∞—Å–∫–æ–ª—å–∫–æ —è–≤–Ω–æ –∏ –ø–æ–¥—Ä–æ–±–Ω–æ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –∫–æ–º–ø–∞–Ω–∏—è (–æ—Ç 0.0 ‚Äî –≤—Å–∫–æ–ª—å–∑—å –¥–æ 1.0 ‚Äî —è–≤–Ω–æ –∏ –ø–æ–¥—Ä–æ–±–Ω–æ)

–û—Ç–≤–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
sentiment: [–∑–Ω–∞—á–µ–Ω–∏–µ]
impact_strength: [–∑–Ω–∞—á–µ–Ω–∏–µ]
mention_focus: [–∑–Ω–∞—á–µ–Ω–∏–µ]

–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏:
{record['news_text']}
'''

    answer = f'''sentiment: {record['sentiment']}
impact_strength: {record['impact_strength']}
mention_focus: {record['mention_focus']}'''

    full_text = prompt.strip() + '\n\n' + answer.strip()

    tokenized = tokenizer(
        full_text,
        truncation=True,
        padding='max_length',
        max_length=1024,
        return_tensors=None  # –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ!
    )

    tokenized['labels'] = tokenized['input_ids'].copy()

    # üî• –í–µ—Ä–Ω—É—Ç—å —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–ª—é—á–∏!
    return {
        'input_ids': tokenized['input_ids'],
        'attention_mask': tokenized['attention_mask'],
        'labels': tokenized['labels']
    }




print('START FINETUNE')

# –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ safetensors
MODEL_PATH = '/models/llama-3-8b-gpt-4o-ru1.0'  # –ú–æ–Ω—Ç–∏—Ä—É–π—Ç–µ –≤ docker-compose –∫–∞–∫ volume

print('MODEL PATH IS DIR', os.path.isdir(MODEL_PATH), MODEL_PATH)


# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    local_files_only=True,
)
print('MODEL LOADED')
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_PATH,
    trust_remote_code=True,
    local_files_only=True,
)
print('TOKENIZER LOADED')

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ CSV
print('\n[INFO] –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...', flush=True)
dataset = load_dataset('csv', data_files={'train': '/models/datasets/news_500.csv'}, delimiter=',')

print('DATASET LOADED', dataset)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
print('\n[INFO] –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA...', flush=True)
lora_config = LoraConfig(
    r=8,
    lora_alpha=32,
    target_modules=['q_proj', 'v_proj'],
    lora_dropout=0.05,
    bias='none',
    task_type='CAUSAL_LM'
)
model = get_peft_model(model, lora_config)

# –ê—Ä–≥—É–º–µ–Ω—Ç—ã –æ–±—É—á–µ–Ω–∏—è
training_args = TrainingArguments(
    output_dir='/finetuned',  # –ú–æ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ volume –≤ docker-compose
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=16,
    save_steps=100,
    save_total_limit=2,
    logging_steps=20,
    learning_rate=2e-4,
    weight_decay=0.01,
    warmup_steps=100,
    logging_dir='/finetuned/logs',
    bf16=False,  # CPU
    fp16=False,
    report_to='none',
    remove_unused_columns=False,
)

# Data collator –¥–ª—è masked language modeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

tokenized_dataset = dataset['train'].map(
    tokenize_function,
    batched=False,
    remove_columns=dataset['train'].column_names  # ‚ùó —É–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ–ª—è –∫—Ä–æ–º–µ input_ids/labels
)

# Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
)

# –°—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è
print('\n[INFO] –ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...', flush=True)
trainer.train()

# –°–æ—Ö—Ä–∞–Ω—è–µ–º LoRA –∞–¥–∞–ø—Ç–µ—Ä—ã
print('\n[INFO] –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å...', flush=True)
model.save_pretrained('/finetuned')
tokenizer.save_pretrained('/finetuned')

print('\n[INFO] –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.', flush=True)
